#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦äº•ç¥ç¤¾è³‡æ–™å¢å¼·å·¥å…· - ChromaDB å‘é‡è³‡æ–™åº«å¯¦ä½œ
è² è²¬å°‡ markdown æª”æ¡ˆæ˜ å°„åˆ° ChromaDB ä¸¦æä¾› GPT-4o-mini å•ç­”åŠŸèƒ½
"""

import os
import re
import json
import logging
import math
import hashlib
import time
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
import openai
from openai import OpenAI
from dotenv import load_dotenv

class ChromaDBManager:
    """ChromaDB å‘é‡è³‡æ–™åº«ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = "./chroma_db", collection_name: str = "fukui_tourism"):
        """åˆå§‹åŒ– ChromaDB ç®¡ç†å™¨
        
        Args:
            db_path: è³‡æ–™åº«å„²å­˜è·¯å¾‘
            collection_name: é›†åˆåç¨±
        """
        self.db_path = Path(db_path)
        self.collection_name = collection_name
        self.setup_logging()
        self.setup_environment()
        self.setup_chromadb()
        self.setup_openai()
        
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('chromadb.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_environment(self):
        """è¼‰å…¥ç’°å¢ƒè®Šæ•¸"""
        load_dotenv()
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        if not self.openai_api_key:
            raise ValueError("æœªæ‰¾åˆ° OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        self.logger.info("ç’°å¢ƒè®Šæ•¸è¼‰å…¥æˆåŠŸ")
        
    def setup_chromadb(self):
        """è¨­å®š ChromaDB"""
        try:
            # å»ºç«‹è³‡æ–™åº«ç›®éŒ„
            self.db_path.mkdir(parents=True, exist_ok=True)
            
            # å˜—è©¦æ¸…ç†å¯èƒ½æå£çš„è³‡æ–™åº«æª”æ¡ˆ
            self._cleanup_corrupted_db()
            
            # ä½¿ç”¨æ–°çš„ ChromaDB å®¢æˆ¶ç«¯åˆå§‹åŒ–æ–¹å¼
            self.client = chromadb.PersistentClient(path=str(self.db_path))
            
            # å˜—è©¦ç²å–ç¾æœ‰é›†åˆæˆ–å»ºç«‹æ–°é›†åˆ
            try:
                self.collection = self.client.get_collection(name=self.collection_name)
                self.logger.info(f"æˆåŠŸè¼‰å…¥ç¾æœ‰é›†åˆï¼š{self.collection_name}")
            except Exception:
                # å¦‚æœé›†åˆä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„é›†åˆ
                self.collection = self.client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "ç¦äº•ç¸£è§€å…‰æ™¯é»å’Œç¥ç¤¾è³‡è¨Š"}
                )
                self.logger.info(f"æˆåŠŸå»ºç«‹æ–°é›†åˆï¼š{self.collection_name}")
            
            self.logger.info(f"ChromaDB åˆå§‹åŒ–æˆåŠŸï¼Œé›†åˆåç¨±ï¼š{self.collection_name}")
            
        except Exception as e:
            self.logger.error(f"ChromaDB åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            # å˜—è©¦æ›´è©³ç´°çš„éŒ¯èª¤è™•ç†
            if "'_type'" in str(e):
                self.logger.error("æª¢æ¸¬åˆ° ChromaDB ç‰ˆæœ¬ç›¸å®¹æ€§å•é¡Œï¼Œå˜—è©¦æ¸…ç†ä¸¦é‡æ–°åˆå§‹åŒ–...")
                self._cleanup_corrupted_db()
                raise Exception("ChromaDB ç‰ˆæœ¬ç›¸å®¹æ€§å•é¡Œï¼Œè«‹é‡æ–°å•Ÿå‹•æœå‹™")
            raise
    
    def _cleanup_corrupted_db(self):
        """æ¸…ç†å¯èƒ½æå£çš„è³‡æ–™åº«æª”æ¡ˆ"""
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰æå£çš„ SQLite æª”æ¡ˆ
            sqlite_file = self.db_path / "chroma.sqlite3"
            if sqlite_file.exists():
                # å‚™ä»½ç¾æœ‰æª”æ¡ˆ
                backup_file = self.db_path / f"chroma_backup_{int(time.time())}.sqlite3"
                sqlite_file.rename(backup_file)
                self.logger.info(f"å·²å‚™ä»½å¯èƒ½æå£çš„è³‡æ–™åº«æª”æ¡ˆï¼š{backup_file}")
            
            # æ¸…ç†å…¶ä»–å¯èƒ½çš„æå£æª”æ¡ˆ
            for file_path in self.db_path.glob("*"):
                if file_path.is_file() and file_path.suffix in ['.sqlite3', '.db']:
                    if file_path.name != 'chroma.sqlite3':
                        backup_file = self.db_path / f"{file_path.stem}_backup_{int(time.time())}{file_path.suffix}"
                        file_path.rename(backup_file)
                        self.logger.info(f"å·²å‚™ä»½æª”æ¡ˆï¼š{backup_file}")
                        
        except Exception as e:
            self.logger.warning(f"æ¸…ç†è³‡æ–™åº«æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            
    def setup_openai(self):
        """è¨­å®š OpenAI å®¢æˆ¶ç«¯"""
        try:
            self.openai_client = OpenAI(api_key=self.openai_api_key)
            self.logger.info("OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            raise
    
    @staticmethod
    def calculate_distance(lat1: float, lng1: float, lat2: float, lng2: float) -> float:
        """è¨ˆç®—å…©å€‹åœ°é»ä¹‹é–“çš„è·é›¢ï¼ˆå…¬é‡Œï¼‰
        
        ä½¿ç”¨ Haversine å…¬å¼è¨ˆç®—çƒé¢è·é›¢
        
        Args:
            lat1, lng1: ç¬¬ä¸€å€‹åœ°é»çš„ç¶“ç·¯åº¦
            lat2, lng2: ç¬¬äºŒå€‹åœ°é»çš„ç¶“ç·¯åº¦
            
        Returns:
            è·é›¢ï¼ˆå…¬é‡Œï¼‰
        """
        # å°‡è§’åº¦è½‰æ›ç‚ºå¼§åº¦
        lat1_rad = math.radians(lat1)
        lng1_rad = math.radians(lng1)
        lat2_rad = math.radians(lat2)
        lng2_rad = math.radians(lng2)
        
        # Haversine å…¬å¼
        dlat = lat2_rad - lat1_rad
        dlng = lng2_rad - lng1_rad
        
        a = (math.sin(dlat / 2) ** 2 + 
             math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlng / 2) ** 2)
        c = 2 * math.asin(math.sqrt(a))
        
        # åœ°çƒåŠå¾‘ï¼ˆå…¬é‡Œï¼‰
        earth_radius = 6371
        
        return earth_radius * c
    
    def extract_location_from_query(self, query: str) -> Optional[Tuple[str, float, float]]:
        """å¾æŸ¥è©¢ä¸­æå–åœ°é»è³‡è¨Š
        
        Args:
            query: ä½¿ç”¨è€…æŸ¥è©¢
            
        Returns:
            (city, latitude, longitude) æˆ– None
        """
        # è¼‰å…¥ä½ç½®è³‡æ–™ä»¥æŸ¥æ‰¾æåˆ°çš„åŸå¸‚
        try:
            import json
            from pathlib import Path
            
            base_path = Path(__file__).parent.parent.parent
            locations_file = base_path / "output" / "fukui_enhanced_locations.json"
            
            if not locations_file.exists():
                return None
                
            with open(locations_file, 'r', encoding='utf-8') as f:
                locations_data = json.load(f)
            
            # å»ºç«‹åŸå¸‚åˆ°åº§æ¨™çš„æ˜ å°„
            city_coords = {}
            for location in locations_data:
                city = location.get('original_data', {}).get('city', '')
                if city and city not in city_coords:
                    # å„ªå…ˆä½¿ç”¨ Google Maps åº§æ¨™
                    google_data = location.get('google_maps_data', {})
                    geometry = google_data.get('geometry', {})
                    if geometry and 'location' in geometry:
                        try:
                            lat = float(geometry['location']['lat'])
                            lng = float(geometry['location']['lng'])
                            city_coords[city] = (lat, lng)
                        except (ValueError, TypeError):
                            pass
                    
                    # å¦‚æœæ²’æœ‰ Google åº§æ¨™ï¼Œä½¿ç”¨åŸå§‹åº§æ¨™
                    if city not in city_coords:
                        original_data = location.get('original_data', {})
                        lat = original_data.get('latitude')
                        lng = original_data.get('longitude')
                        if lat is not None and lng is not None:
                            try:
                                city_coords[city] = (float(lat), float(lng))
                            except (ValueError, TypeError):
                                pass
            
            # æª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«åŸå¸‚åç¨±
            for city, (lat, lng) in city_coords.items():
                if city in query:
                    return city, lat, lng
                    
        except Exception as e:
            self.logger.warning(f"æå–ä½ç½®è³‡è¨Šå¤±æ•—: {e}")
        
        return None
    
    def search_similar_with_location(self, query: str, n_results: int = 5, 
                                   same_city_weight: float = 2.0, 
                                   distance_threshold_km: float = 20.0) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸ä¼¼æ–‡ä»¶ï¼Œè€ƒæ…®åœ°ç†ä½ç½®æ¥è¿‘æ€§
        
        Args:
            query: æŸ¥è©¢å­—ä¸²
            n_results: è¿”å›çµæœæ•¸é‡
            same_city_weight: åŒåŸå¸‚åŠ æ¬Šä¿‚æ•¸
            distance_threshold_km: è·é›¢é–¾å€¼ï¼ˆå…¬é‡Œï¼‰
            
        Returns:
            æœå°‹çµæœï¼ŒæŒ‰ç›¸é—œæ€§å’Œåœ°ç†ä½ç½®æ’åº
        """
        try:
            # å…ˆé€²è¡Œä¸€èˆ¬çš„å‘é‡æœå°‹ï¼Œç²å–æ›´å¤šå€™é¸çµæœ
            initial_results = self.collection.query(
                query_texts=[query],
                n_results=min(n_results * 3, 20),  # ç²å–æ›´å¤šå€™é¸çµæœ
                include=["documents", "metadatas", "distances"]
            )
            
            # å˜—è©¦å¾æŸ¥è©¢ä¸­æå–åœ°é»è³‡è¨Š
            query_location = self.extract_location_from_query(query)
            
            formatted_results = []
            for i in range(len(initial_results["documents"][0])):
                result = {
                    "content": initial_results["documents"][0][i],
                    "metadata": initial_results["metadatas"][0][i],
                    "distance": initial_results["distances"][0][i],
                    "location_score": 0  # åœ°ç†ä½ç½®å¾—åˆ†
                }
                
                # è¨ˆç®—åœ°ç†ä½ç½®å¾—åˆ†
                if query_location:
                    query_city, query_lat, query_lng = query_location
                    result_metadata = result["metadata"]
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºåŒä¸€åŸå¸‚
                    result_city = result_metadata.get('city', '')
                    if result_city == query_city:
                        result["location_score"] = same_city_weight
                    else:
                        # è¨ˆç®—è·é›¢ä¸¦çµ¦äºˆè·é›¢è©•åˆ†
                        try:
                            # å˜—è©¦å¾ metadata ä¸­ç²å–åº§æ¨™
                            result_lat = result_metadata.get('latitude')
                            result_lng = result_metadata.get('longitude')
                            
                            if result_lat is not None and result_lng is not None:
                                result_lat = float(result_lat)
                                result_lng = float(result_lng)
                                
                                distance = self.calculate_distance(
                                    query_lat, query_lng, result_lat, result_lng
                                )
                                
                                # è·é›¢è¶Šè¿‘ï¼Œå¾—åˆ†è¶Šé«˜
                                if distance <= distance_threshold_km:
                                    result["location_score"] = max(0, (distance_threshold_km - distance) / distance_threshold_km)
                                    
                        except (ValueError, TypeError) as e:
                            self.logger.debug(f"ç„¡æ³•è¨ˆç®—è·é›¢: {e}")
                
                formatted_results.append(result)
            
            # æ ¹æ“šçµ„åˆå¾—åˆ†æ’åºï¼ˆå‘é‡ç›¸ä¼¼åº¦ + åœ°ç†ä½ç½®å¾—åˆ†ï¼‰
            # è·é›¢è¶Šå°è¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥ä½¿ç”¨è² å€¼ï¼›ä½ç½®å¾—åˆ†è¶Šé«˜è¶Šå¥½
            formatted_results.sort(key=lambda x: (-x["location_score"], x["distance"]))
            
            # è¿”å›å‰ n_results å€‹çµæœ
            return formatted_results[:n_results]
            
        except Exception as e:
            self.logger.error(f"åœ°ç†ä½ç½®æœå°‹å¤±æ•—: {e}")
            # å¦‚æœåœ°ç†æœå°‹å¤±æ•—ï¼Œå›é€€åˆ°ä¸€èˆ¬æœå°‹
            return self.search_similar(query, n_results)
    
    def parse_markdown_sections(self, content: str, source_type: str) -> List[Dict[str, Any]]:
        """è§£æ Markdown å…§å®¹ç‚ºçµæ§‹åŒ–è³‡æ–™
        
        Args:
            content: Markdown å…§å®¹
            source_type: ä¾†æºé¡å‹ ('locations' æˆ– 'shrines')
            
        Returns:
            è§£æå¾Œçš„çµæ§‹åŒ–è³‡æ–™æ¸…å–®
        """
        sections = []
        
        # åˆ†å‰²å„å€‹æ™¯é»/ç¥ç¤¾å€å¡Š
        if source_type == "locations":
            pattern = r'## (\d+)\. (.+?)\n'
        else:  # shrines
            pattern = r'## (\d+)\. (.+?)\n'
            
        matches = list(re.finditer(pattern, content))
        
        for i, match in enumerate(matches):
            start_pos = match.start()
            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(content)
            
            section_content = content[start_pos:end_pos].strip()
            section_data = self.extract_section_data(section_content, source_type)
            
            if section_data:
                sections.append(section_data)
                
        return sections
    
    def extract_section_data(self, section_content: str, source_type: str) -> Optional[Dict[str, Any]]:
        """å¾å€å¡Šå…§å®¹æå–çµæ§‹åŒ–è³‡æ–™
        
        Args:
            section_content: å€å¡Šå…§å®¹
            source_type: ä¾†æºé¡å‹
            
        Returns:
            çµæ§‹åŒ–è³‡æ–™å­—å…¸
        """
        try:
            data = {"source_type": source_type}
            
            # æå–æ¨™é¡Œ
            title_match = re.search(r'## \d+\. (.+)', section_content)
            if title_match:
                data["title"] = title_match.group(1).strip()
                self.logger.debug(f"æå–æ¨™é¡ŒæˆåŠŸ: {data['title']}")
            else:
                self.logger.warning(f"ç„¡æ³•æå–æ¨™é¡Œï¼Œå…§å®¹é–‹é ­: {section_content[:100]}...")
            
            # æå–åŸºæœ¬è³‡è¨Šè¡¨æ ¼
            basic_info_pattern = r'\| é …ç›® \| å…§å®¹ \|\n\|------|------\|(.*?)\n\n'
            basic_info_match = re.search(basic_info_pattern, section_content, re.DOTALL)
            
            if basic_info_match:
                table_content = basic_info_match.group(1)
                if table_content:  # ç¢ºä¿ table_content ä¸æ˜¯ None
                    for row in table_content.split('\n'):
                        if '|' in row:
                            parts = [part.strip() for part in row.split('|') if part.strip()]
                            if len(parts) >= 2:
                                key = parts[0].strip()
                                value = parts[1].strip()
                                if key and value:  # ç¢ºä¿ key å’Œ value éƒ½ä¸æ˜¯ç©ºå­—ä¸²
                                    data[key] = value
                else:
                    self.logger.warning(f"åŸºæœ¬è³‡è¨Šè¡¨æ ¼å…§å®¹ç‚ºç©º")
            else:
                self.logger.warning(f"ç„¡æ³•æ‰¾åˆ°åŸºæœ¬è³‡è¨Šè¡¨æ ¼")
            
            # æå–è©³ç´°æè¿°
            desc_match = re.search(r'### è©³ç´°æè¿°\n\n(.*?)(?=\n### |$)', section_content, re.DOTALL)
            if desc_match:
                data["detailed_description"] = desc_match.group(1).strip()
            
            # æå–ç‡Ÿæ¥­æ™‚é–“ï¼ˆåƒ…é™æ™¯é»ï¼‰
            if source_type == "locations":
                hours_match = re.search(r'### ç‡Ÿæ¥­æ™‚é–“\n\n(.*?)(?=\n### |$)', section_content, re.DOTALL)
                if hours_match:
                    data["operating_hours"] = hours_match.group(1).strip()
            
            # æå–è©•è«–
            reviews_match = re.search(r'### éŠå®¢è©•è«–ç²¾é¸\n\n(.*?)(?=\n\*\*é—œéµæ¨™ç±¤|$)', section_content, re.DOTALL)
            if reviews_match:
                data["reviews"] = reviews_match.group(1).strip()
            
            # æå–é—œéµæ¨™ç±¤ - æ”¹é€²æ­£å‰‡è¡¨é”å¼ä»¥é©æ‡‰ä¸åŒæ ¼å¼
            tags_match = re.search(r'\*\*é—œéµæ¨™ç±¤ï¼š\*\* (.+?)(?:\n|$)', section_content, re.DOTALL)
            if tags_match:
                tags_text = tags_match.group(1).strip()
                if tags_text:  # ç¢ºä¿ tags_text ä¸æ˜¯ None æˆ–ç©ºå­—ä¸²
                    # ç§»é™¤å¯èƒ½çš„çµå°¾ç¬¦è™Ÿå’Œå¤šé¤˜ç©ºæ ¼
                    tags_text = tags_text.replace('\n', ' ').strip()
                    data["tags"] = [tag.strip() for tag in tags_text.split('|') if tag.strip()]
                    self.logger.debug(f"æå–æ¨™ç±¤æˆåŠŸ: {data['tags']}")
                else:
                    data["tags"] = []
            else:
                data["tags"] = []
                self.logger.warning(f"ç„¡æ³•æ‰¾åˆ°é—œéµæ¨™ç±¤")
            
            # å»ºç«‹å®Œæ•´æ–‡æœ¬ç”¨æ–¼åµŒå…¥
            data["full_text"] = section_content
            
            return data
            
        except Exception as e:
            self.logger.error(f"è§£æå€å¡Šè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            self.logger.error(f"å•é¡Œå…§å®¹å‰100å­—ç¬¦: {section_content[:100] if section_content else 'None'}")
            return None
    
    def load_and_process_files(self, locations_file: str, shrines_file: str) -> Tuple[List[Dict], List[Dict]]:
        """è¼‰å…¥ä¸¦è™•ç† markdown æª”æ¡ˆ
        
        Args:
            locations_file: æ™¯é»æª”æ¡ˆè·¯å¾‘
            shrines_file: ç¥ç¤¾æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è™•ç†å¾Œçš„æ™¯é»å’Œç¥ç¤¾è³‡æ–™
        """
        locations_data = []
        shrines_data = []
        
        # è™•ç†æ™¯é»æª”æ¡ˆ
        try:
            with open(locations_file, 'r', encoding='utf-8') as f:
                locations_content = f.read()
            locations_data = self.parse_markdown_sections(locations_content, "locations")
            self.logger.info(f"æˆåŠŸè§£æ {len(locations_data)} å€‹æ™¯é»")
        except Exception as e:
            self.logger.error(f"è¼‰å…¥æ™¯é»æª”æ¡ˆå¤±æ•—ï¼š{e}")
        
        # è™•ç†ç¥ç¤¾æª”æ¡ˆ
        try:
            with open(shrines_file, 'r', encoding='utf-8') as f:
                shrines_content = f.read()
            shrines_data = self.parse_markdown_sections(shrines_content, "shrines")
            self.logger.info(f"æˆåŠŸè§£æ {len(shrines_data)} å€‹ç¥ç¤¾")
        except Exception as e:
            self.logger.error(f"è¼‰å…¥ç¥ç¤¾æª”æ¡ˆå¤±æ•—ï¼š{e}")
        
        return locations_data, shrines_data
    
    def generate_document_id(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡ä»¶å”¯ä¸€ ID
        
        Args:
            data: æ–‡ä»¶è³‡æ–™
            
        Returns:
            å”¯ä¸€ ID
        """
        # ä½¿ç”¨æ›´å¤šæ¬„ä½ä¾†ç¢ºä¿ ID çš„å”¯ä¸€æ€§
        source_type = data.get('source_type', '')
        title = data.get('title', '')
        address = data.get('åœ°å€', data.get('æ‰€åœ¨åœ°', ''))
        full_text_hash = hashlib.md5(data.get('full_text', '').encode('utf-8')).hexdigest()[:8]
        
        content = f"{source_type}-{title}-{address}-{full_text_hash}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def insert_documents(self, documents: List[Dict[str, Any]]) -> bool:
        """å°‡æ–‡ä»¶æ’å…¥ ChromaDB
        
        Args:
            documents: æ–‡ä»¶æ¸…å–®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            if not documents:
                self.logger.warning("æ²’æœ‰æ–‡ä»¶éœ€è¦æ’å…¥")
                return True
                
            ids = []
            texts = []
            metadatas = []
            
            for doc in documents:
                doc_id = self.generate_document_id(doc)
                ids.append(doc_id)
                texts.append(doc.get("full_text", ""))
                
                # å»ºç«‹ metadataï¼ˆç§»é™¤ full_text é¿å…é‡è¤‡ï¼‰
                metadata = {k: v for k, v in doc.items() if k != "full_text"}
                # ç¢ºä¿æ‰€æœ‰å€¼éƒ½æ˜¯å­—ç¬¦ä¸²é¡å‹
                for key, value in metadata.items():
                    if isinstance(value, list):
                        metadata[key] = " | ".join(str(v) for v in value)
                    else:
                        metadata[key] = str(value)
                metadatas.append(metadata)
            
            # æ‰¹æ¬¡æ’å…¥
            self.collection.add(
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            self.logger.info(f"æˆåŠŸæ’å…¥ {len(documents)} å€‹æ–‡ä»¶")
            return True
            
        except Exception as e:
            self.logger.error(f"æ’å…¥æ–‡ä»¶å¤±æ•—ï¼š{e}")
            return False
    
    def search_similar(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """æœå°‹ç›¸ä¼¼æ–‡ä»¶
        
        Args:
            query: æŸ¥è©¢å­—ä¸²
            n_results: è¿”å›çµæœæ•¸é‡
            
        Returns:
            æœå°‹çµæœ
        """
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            formatted_results = []
            for i in range(len(results["documents"][0])):
                formatted_results.append({
                    "content": results["documents"][0][i],
                    "metadata": results["metadatas"][0][i],
                    "distance": results["distances"][0][i]
                })
            
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"æœå°‹å¤±æ•—ï¼š{e}")
            return []
    
    def ask_gpt(self, question: str, context_docs: List[Dict[str, Any]] = None, use_location_aware_search: bool = True) -> str:
        """ä½¿ç”¨ GPT-4o-mini å›ç­”å•é¡Œ
        
        Args:
            question: å•é¡Œ
            context_docs: ä¸Šä¸‹æ–‡æ–‡ä»¶
            use_location_aware_search: æ˜¯å¦ä½¿ç”¨åœ°ç†ä½ç½®æ„ŸçŸ¥æœå°‹
            
        Returns:
            GPT å›ç­”
        """
        try:
            # å¦‚æœæ²’æœ‰æä¾›ä¸Šä¸‹æ–‡ï¼Œå…ˆæœå°‹ç›¸é—œæ–‡ä»¶
            if not context_docs:
                if use_location_aware_search:
                    context_docs = self.search_similar_with_location(question, n_results=3)
                else:
                    context_docs = self.search_similar(question, n_results=3)
            
            # å»ºç«‹ä¸Šä¸‹æ–‡å­—ä¸²
            context_text = "\n\n".join([
                f"ã€{doc['metadata'].get('title', 'Unknown')}ã€‘\n{doc['content'][:1000]}..."
                for doc in context_docs
            ])
            
            # å»ºç«‹ç³»çµ±æç¤º - å°ˆæ¥­å°éŠè§’è‰²ï¼Œå¼·èª¿åœ°ç†ä½ç½®ç›¸è¿‘çš„æ¨è–¦
            system_prompt = """You are an experienced and enthusiastic tour guide specializing in Fukui Prefecture, Japan. You have extensive knowledge about local attractions, shrines, temples, culture, and travel tips.

Your responsibilities:
- Act as a professional, friendly, and knowledgeable tour guide
- Provide detailed, engaging, and well-organized information
- Include practical travel advice when relevant (opening hours, best times to visit, etc.)
- Share interesting historical and cultural context
- Use a warm, welcoming tone that makes tourists excited about visiting
- Structure your responses clearly with headings, bullet points, and organized sections when appropriate
- Provide specific recommendations and insider tips

**IMPORTANT: Geographic Location Priority**
When recommending attractions or places to visit:
1. **PRIORITIZE locations in the same city/area** - Group recommendations by geographic proximity
2. **Consider travel convenience** - Suggest locations that are close to each other for efficient sightseeing
3. **Mention geographic relationships** - Clearly state when attractions are in the same city or nearby areas
4. **Create logical travel routes** - When possible, suggest visiting nearby attractions together
5. **Include distance/travel time information** - Help tourists understand the geographic context

Always respond in English with enthusiasm and expertise, as if you're personally guiding tourists through Fukui Prefecture."""
            
            # å»ºç«‹ä½¿ç”¨è€…æç¤º - æ›´å°ˆæ¥­çš„æ ¼å¼ï¼Œå¼·èª¿åœ°ç†ä½ç½®
            user_prompt = f"""Based on the following tourism information about Fukui Prefecture, please provide a comprehensive and engaging response as a professional tour guide:

ã€Tourism Informationã€‘
{context_text}

ã€Tourist Questionã€‘
{question}

**Geographic Context Instructions:**
- Pay attention to the city/location information in each attraction's metadata
- Group and prioritize recommendations by geographic proximity (same city or nearby areas)
- When suggesting multiple attractions, organize them geographically for efficient travel planning
- Mention which attractions are in the same city or area for convenient joint visits

Please structure your response professionally with:
1. A welcoming introduction if appropriate
2. Detailed information organized by geographic areas/cities when possible
3. Practical travel tips and recommendations including geographic convenience
4. Cultural or historical insights
5. Best times to visit or special considerations
6. Geographic groupings of nearby attractions for efficient sightseeing

Format your response using proper Markdown formatting including:
- Use **bold text** for important points and attraction names
- Use bullet points (- or *) for lists and key features  
- Use ## for main section headings when organizing content (consider geographic groupings)
- Use ### for sub-sections if needed
- Use > for special tips or quotes about travel convenience

Make it informative, engaging, and helpful for tourists planning their visit to Fukui Prefecture with efficient geographic routing:"""

            # å‘¼å« GPT-4o-mini å…·æœ‰æ›´è‡ªç„¶çš„åƒæ•¸è¨­å®š
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.8,  # æé«˜å‰µæ„æ€§ï¼Œè®“å›æ‡‰æ›´è‡ªç„¶æœ‰è¶£
                max_tokens=2000,  # å¢åŠ é•·åº¦é™åˆ¶ä»¥æ”¯æ´æ›´è©³ç´°çš„å›æ‡‰
                top_p=0.9,  # æ–°å¢ top_p åƒæ•¸ä»¥æå‡å›æ‡‰å“è³ª
                frequency_penalty=0.1,  # è¼•å¾®æ¸›å°‘é‡è¤‡
                presence_penalty=0.1   # é¼“å‹µå¤šæ¨£åŒ–è¡¨é”
            )
            
            answer = response.choices[0].message.content
            self.logger.info(f"GPT å›ç­”ç”ŸæˆæˆåŠŸï¼Œå•é¡Œï¼š{question[:50]}...")
            return answer
            
        except Exception as e:
            self.logger.error(f"GPT answer generation failed: {e}")
            return f"Sorry, an error occurred while generating the answer: {str(e)}"
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """å–å¾—é›†åˆçµ±è¨ˆè³‡è¨Š
        
        Returns:
            çµ±è¨ˆè³‡è¨Š
        """
        try:
            count = self.collection.count()
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_function": "text-embedding-3-small"
            }
        except Exception as e:
            self.logger.error(f"å–å¾—çµ±è¨ˆè³‡è¨Šå¤±æ•—ï¼š{e}")
            return {}

def main():
    """ä¸»å‡½å¼ - å±•ç¤ºç³»çµ±ä½¿ç”¨ç¯„ä¾‹"""
    try:
        # åˆå§‹åŒ– ChromaDB ç®¡ç†å™¨
        print("ğŸš€ åˆå§‹åŒ– ChromaDB å‘é‡è³‡æ–™åº«...")
        chroma_manager = ChromaDBManager()
        
        # å®šç¾©æª”æ¡ˆè·¯å¾‘
        locations_file = "/Users/zhuboyuan/Desktop/University-NCHU/NCHU-Project/Project-FUKUI/src/src-LLM-Shrine/output/locations_natural_language.md"
        shrines_file = "/Users/zhuboyuan/Desktop/University-NCHU/NCHU-Project/Project-FUKUI/src/src-LLM-Shrine/output/shrines_natural_language.md"
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(locations_file):
            print(f"âŒ æ™¯é»æª”æ¡ˆä¸å­˜åœ¨ï¼š{locations_file}")
            return
        if not os.path.exists(shrines_file):
            print(f"âŒ ç¥ç¤¾æª”æ¡ˆä¸å­˜åœ¨ï¼š{shrines_file}")
            return
        
        # è¼‰å…¥ä¸¦è™•ç†æª”æ¡ˆ
        print("ğŸ“š è¼‰å…¥ä¸¦è™•ç† Markdown æª”æ¡ˆ...")
        locations_data, shrines_data = chroma_manager.load_and_process_files(
            locations_file, shrines_file
        )
        
        # åˆä½µæ‰€æœ‰è³‡æ–™
        all_documents = locations_data + shrines_data
        print(f"ğŸ“Š ç¸½å…±è™•ç†äº† {len(all_documents)} å€‹æ–‡ä»¶")
        
        # æ’å…¥è³‡æ–™åº«
        print("ğŸ’¾ å°‡è³‡æ–™æ’å…¥ ChromaDB...")
        success = chroma_manager.insert_documents(all_documents)
        
        if success:
            print("âœ… è³‡æ–™åº«å»ºç«‹æˆåŠŸï¼")
            
            # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
            stats = chroma_manager.get_collection_stats()
            print(f"ğŸ“ˆ è³‡æ–™åº«çµ±è¨ˆï¼š{stats}")
            
            # æ¸¬è©¦å•ç­”åŠŸèƒ½
            print("\nğŸ¤– æ¸¬è©¦ GPT-4o-mini å•ç­”åŠŸèƒ½...")
            test_questions = [
                "ç¦äº•ç¸£æœ‰å“ªäº›è‘—åçš„ç¥ç¤¾ï¼Ÿ",
                "æ¨è–¦ä¸€äº›ç¦äº•ç¸£çš„æµ·å²¸æ™¯é»",
                "æ°¸å¹³å¯ºçš„ç‰¹è‰²æ˜¯ä»€éº¼ï¼Ÿ"
            ]
            
            for question in test_questions:
                print(f"\nâ“ å•é¡Œï¼š{question}")
                answer = chroma_manager.ask_gpt(question)
                print(f"ğŸ’¬ å›ç­”ï¼š{answer}\n" + "="*60)
                
        else:
            print("âŒ è³‡æ–™åº«å»ºç«‹å¤±æ•—")
            
    except Exception as e:
        print(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        logging.error(f"ä¸»ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    main()
