# %%
import pandas as pd
import json
import requests
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import os
from datetime import datetime
import hashlib
from googleapiclient.discovery import build
from dotenv import load_dotenv

# è¼‰å…¥ .env æª”æ¡ˆ
load_dotenv()

# API è¨­å®š - ä½¿ç”¨ç’°å¢ƒè®Šæ•¸ä¿è­· API é‡‘é‘°
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_ENGINE_ID = os.getenv("GOOGLE_ENGINE_ID")

@dataclass
class ShrineInfo:
    """ç¥ç¤¾åŸºæœ¬è³‡è¨Šçµæ§‹"""
    # åŸºæœ¬è­˜åˆ¥è³‡è¨Š
    name_jp: str
    name_en: str
    romaji: str
    type: str  # ç¥ç¤¾/å¯º
    
    # ä½ç½®åº§æ¨™
    prefecture: str
    city: str
    address: str
    lat: float
    lon: float
    geohash: str
    
    # äº¤é€šæŒ‡å¼•
    nearest_station: str
    access_time_walk: str
    bus_info: str
    parking: str
    
    # æ­·å²èˆ‡æ–‡åŒ–èƒŒæ™¯
    founded_year: str
    founder: str
    historical_events: List[str]
    important_cultural_property: List[str]
    unesco: bool
    architectural_style: str
    enshrined_deities: List[Dict[str, str]]  # [{"name": "ç¥æ˜", "role": "åŠŸå¾·"}]
    
    # ç¥ˆé¡˜èˆ‡æœå‹™
    prayer_categories: List[str]
    omamori_types: List[str]
    goshuin: bool
    ceremonies: List[Dict[str, Any]]  # [{"name": "å„€å¼å", "reservation_req": bool, "fee": int}]
    
    # åƒæ‹œè³‡è¨Š
    gate_open: str
    gate_close: str
    office_hours: str
    admission_fee: int  # JPY
    annual_festivals: List[Dict[str, str]]  # [{"name": "ç¥­å…¸å", "date": "æ—¥æœŸ", "description": "ç°¡è¿°"}]
    
    # æ—…éŠé«”é©— & ä¾¿åˆ©è¨­æ–½
    highlights: List[str]
    best_seasons: List[str]
    wheelchair_access: bool
    toilets: bool
    wifi: bool
    photo_policy: str
    
    # é¡å¤–è³‡è¨Š
    description: str
    phone: str
    url: str
    
    # ä¾†æºè³‡è¨Š
    sources: List[Dict[str, str]]  # [{"title": "æ¨™é¡Œ", "url": "ç¶²å€", "snippet": "æ‘˜è¦", "source": "ä¾†æºé¡å‹"}]

class ShrineDataEnhancer:
    """ç¥ç¤¾è³‡æ–™å¢å¼·å™¨"""
    
    def __init__(self, perplexity_api_key: str, openai_api_key: str, google_api_key: str, google_engine_id: str):
        self.perplexity_api_key = perplexity_api_key
        self.openai_api_key = openai_api_key
        self.google_api_key = google_api_key
        self.google_engine_id = google_engine_id
        
        self.perplexity_headers = {
            "Authorization": f"Bearer {perplexity_api_key}",
            "Content-Type": "application/json"
        }
        self.openai_headers = {
            "Authorization": f"Bearer {openai_api_key}",
            "Content-Type": "application/json"
        }
        
        # åˆå§‹åŒ– Google Custom Search
        self.google_service = build("customsearch", "v1", developerKey=google_api_key)
    
    def search_shrine_info_with_perplexity(self, shrine_name: str, address: str) -> str:
        """ä½¿ç”¨ Perplexity API æœå°‹ç¥ç¤¾è©³ç´°è³‡è¨Š"""
        # æå–åœ°å€ä¸­çš„é—œéµåœ°ç†è³‡è¨Š
        city_info = self._extract_location_info(address)
        
        # å¼·åŒ–æŸ¥è©¢ï¼Œæ˜ç¢ºæŒ‡å®šåœ°å€ä½ç½®ä»¥å€åˆ†åŒåç¥ç¤¾
        query = f'"{shrine_name}" "{address}" {city_info} ç¥ç¤¾ å¯º æ­·å² åƒæ‹œæ™‚é–“ ç¥­å…¸ å¾¡å®ˆ å¾¡æœ±å° äº¤é€š æœ€è¿‘è»Šç«™ å»ºç¯‰æ¨£å¼ ç¥­ç¥ æ–‡åŒ–è²¡'
        
        payload = {
            "model": "sonar",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½æ—¥æœ¬ç¥ç¤¾å¯ºå»Ÿå°ˆå®¶ã€‚è«‹æ ¹æ“šæœå°‹çµæœæä¾›è©³ç´°çš„ç¥ç¤¾è³‡è¨Šï¼ŒåŒ…æ‹¬æ­·å²èƒŒæ™¯ã€å»ºç¯‰ç‰¹è‰²ã€ä¸»è¦ç¥ä½›ã€åƒæ‹œè³‡è¨Šã€äº¤é€šæ–¹å¼ã€ç¥­å…¸æ´»å‹•ã€æ–‡åŒ–è²¡ç”¢ç­‰ã€‚è«‹ä»¥ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ç›¡å¯èƒ½æä¾›æº–ç¢ºçš„è³‡è¨Šã€‚ç‰¹åˆ¥æ³¨æ„ï¼šæ—¥æœ¬æœ‰å¾ˆå¤šåŒåç¥ç¤¾ï¼Œè«‹ç¢ºèªæœå°‹çš„æ˜¯æŒ‡å®šåœ°å€çš„ç¥ç¤¾ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è«‹æä¾›é—œæ–¼{shrine_name}çš„è©³ç´°è³‡è¨Šï¼Œç‰¹åˆ¥æ³¨æ„é€™æ˜¯ä½æ–¼ã€Œ{address}ã€çš„ç¥ç¤¾ï¼ˆä¸æ˜¯å…¶ä»–åœ°å€çš„åŒåç¥ç¤¾ï¼‰ã€‚è«‹åŒ…æ‹¬ï¼š1.æ­·å²æ²¿é©èˆ‡å‰µå»ºå¹´ä»½ 2.ä¸»è¦ç¥­ç¥èˆ‡åŠŸå¾· 3.å»ºç¯‰æ¨£å¼èˆ‡æ–‡åŒ–è²¡ç”¢ 4.åƒæ‹œæ™‚é–“èˆ‡é–€ç¥¨è²»ç”¨ 5.äº¤é€šæ–¹å¼èˆ‡æœ€è¿‘è»Šç«™ 6.ä¸»è¦ç¥­å…¸èˆ‡æ´»å‹• 7.å¾¡å®ˆèˆ‡å¾¡æœ±å°è³‡è¨Š 8.çœ‹é»èˆ‡å­£ç¯€ç‰¹è‰² 9.ä¾¿æ°‘è¨­æ–½ã€‚å¦‚æœæ‰¾ä¸åˆ°è©²ç‰¹å®šåœ°å€çš„ç¥ç¤¾è³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        try:
            response = requests.post(
                "https://api.perplexity.ai/chat/completions",
                headers=self.perplexity_headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                return "ç„¡æ³•ç²å–è©³ç´°è³‡è¨Š"
                
        except Exception as e:
            print(f"Perplexity API éŒ¯èª¤: {e}")
            return f"æœå°‹éŒ¯èª¤: {str(e)}"
    
    def search_shrine_info_with_google(self, shrine_name: str, address: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Google Custom Search API æœå°‹ç¥ç¤¾è©³ç´°è³‡è¨Š"""
        # æå–åœ°å€ä¸­çš„é—œéµåœ°ç†è³‡è¨Š
        city_info = self._extract_location_info(address)
        
        # å‰µå»ºå¤šå€‹ç²¾ç¢ºæœå°‹ç­–ç•¥ï¼Œå„ªå…ˆä½¿ç”¨å®Œæ•´åœ°å€
        queries = [
            f'"{shrine_name}" "{address}"',  # æœ€ç²¾ç¢ºï¼šç¥ç¤¾åç¨±å’Œå®Œæ•´åœ°å€
            f'"{shrine_name}" {city_info}',  # ç¥ç¤¾åç¨±å’ŒåŸå¸‚è³‡è¨Š
            f'{shrine_name} {address.split()[0] if address.split() else address}',  # ç¥ç¤¾åç¨±å’Œåœ°å€ç¬¬ä¸€éƒ¨åˆ†
            f'{shrine_name} {city_info} ç¥ç¤¾',  # å‚³çµ±æœå°‹æ–¹å¼ä½œç‚ºå‚™æ¡ˆ
        ]
        
        try:
            search_results = []
            combined_content = []
            
            # å˜—è©¦å¤šå€‹æœå°‹æŸ¥è©¢
            for query in queries:
                try:
                    print(f"    â†’ Google æœå°‹æŸ¥è©¢: {query}")
                    result = self.google_service.cse().list(
                        q=query,
                        cx=self.google_engine_id,
                        num=5,  # æ¯å€‹æŸ¥è©¢ç²å–5å€‹çµæœ
                        lr='lang_ja',  # é™åˆ¶æ—¥æ–‡æœå°‹
                        hl='ja'
                    ).execute()
                    
                    if 'items' in result:
                        print(f"    â†’ æ‰¾åˆ° {len(result['items'])} å€‹æœå°‹çµæœ")
                        valid_results = 0
                        for item in result['items']:
                            # æª¢æŸ¥æœå°‹çµæœæ˜¯å¦èˆ‡ç›®æ¨™åœ°å€ç›¸é—œ
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            url = item.get('link', '')
                            
                            # ç°¡å–®çš„åœ°å€ç›¸é—œæ€§æª¢æŸ¥
                            if self._is_relevant_result(title + snippet, address, city_info):
                                search_result = {
                                    "title": title,
                                    "url": url,
                                    "snippet": snippet,
                                    "source": "Google"
                                }
                                search_results.append(search_result)
                                
                                # çµ„åˆæœå°‹å…§å®¹ç”¨æ–¼ AI åˆ†æ
                                content_piece = f"æ¨™é¡Œ: {title}\nç¶²å€: {url}\næ‘˜è¦: {snippet}\n"
                                combined_content.append(content_piece)
                                valid_results += 1
                        
                        print(f"    â†’ å…¶ä¸­ {valid_results} å€‹çµæœèˆ‡ç›®æ¨™åœ°å€ç›¸é—œ")
                        
                        # å¦‚æœæ‰¾åˆ°è¶³å¤ çš„ç›¸é—œçµæœå°±è·³å‡º
                        if valid_results >= 3:
                            break
                    else:
                        print(f"    â†’ æŸ¥è©¢ '{query}' æ²’æœ‰æ‰¾åˆ°çµæœ")
                        
                except Exception as query_error:
                    print(f"    â†’ æœå°‹æŸ¥è©¢ '{query}' å¤±æ•—: {query_error}")
                    continue
            
            print(f"    â†’ Google æœå°‹å®Œæˆï¼Œå…±ç²å¾— {len(search_results)} å€‹ç›¸é—œçµæœ")
            return {
                "search_results": search_results,
                "combined_content": "\n".join(combined_content) if combined_content else "æœªæ‰¾åˆ°ç›¸é—œæœå°‹çµæœ"
            }
            
        except Exception as e:
            print(f"    â†’ Google Search API éŒ¯èª¤: {e}")
            return {
                "search_results": [],
                "combined_content": f"Googleæœå°‹éŒ¯èª¤: {str(e)}"
            }
    
    def comprehensive_search(self, shrine_name: str, address: str) -> Dict[str, Any]:
        """ç¶œåˆæœå°‹ï¼šçµåˆ Perplexity å’Œ Google Search"""
        print("    â†’ ä½¿ç”¨ Perplexity æœå°‹...")
        perplexity_info = self.search_shrine_info_with_perplexity(shrine_name, address)
        
        print("    â†’ ä½¿ç”¨ Google Search æœå°‹...")
        google_results = self.search_shrine_info_with_google(shrine_name, address)
        
        # çµ„åˆæ‰€æœ‰è³‡è¨Š
        combined_info = f"""
=== Perplexity æœå°‹çµæœ ===
{perplexity_info}

=== Google Search æœå°‹çµæœ ===
{google_results['combined_content']}
"""
        
        # å»ºç«‹ä¾†æºè³‡è¨Š
        perplexity_sources = [{"title": f"{shrine_name} - Perplexity ç¶œåˆè³‡æ–™", "url": "https://perplexity.ai", "snippet": "ä¾†è‡ª Perplexity AI çš„ç¶œåˆæœå°‹çµæœ", "source": "Perplexity"}]
        
        # ç¢ºä¿ Google æœå°‹çµæœè¢«æ­£ç¢ºè™•ç†
        google_sources = google_results['search_results'] if google_results['search_results'] else []
        
        return {
            "combined_info": combined_info,
            "all_sources": perplexity_sources + google_sources
        }
    
    def enhance_description_with_chatgpt(self, raw_info: str, shrine_name: str) -> str:
        """ä½¿ç”¨ ChatGPT API æ½¤é£¾ç¥ç¤¾ä»‹ç´¹"""
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ—…éŠæ–‡æ¡ˆç·¨è¼¯ï¼Œæ“…é•·å°‡ç¥ç¤¾å¯ºå»Ÿçš„è³‡è¨Šæ•´ç†æˆå„ªç¾ã€å¸å¼•äººä¸”è³‡è¨Šè±å¯Œçš„ä»‹ç´¹æ–‡ã€‚è«‹ä¿æŒè³‡è¨Šçš„æº–ç¢ºæ€§ï¼Œä¸¦ä½¿ç”¨å„ªé›…çš„ç¹é«”ä¸­æ–‡ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è«‹å°‡ä»¥ä¸‹é—œæ–¼{shrine_name}çš„è³‡è¨Šæ•´ç†æˆä¸€æ®µå„ªç¾ã€è©³ç´°çš„ä»‹ç´¹æ–‡å­—ã€‚è«‹ä¿æŒæ‰€æœ‰é‡è¦è³‡è¨Šï¼Œä¸¦è®“æ–‡å­—æ›´å…·å¸å¼•åŠ›å’Œå¯è®€æ€§ï¼š\n\n{raw_info}"
                }
            ],
            "max_tokens": 1500,
            "temperature": 0.3
        }
        
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=self.openai_headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content'].strip()
            else:
                return raw_info
                
        except Exception as e:
            print(f"OpenAI API éŒ¯èª¤: {e}")
            return raw_info
    
    def extract_structured_data_with_chatgpt(self, raw_info: str, shrine_name: str, address: str, lat: float, lon: float, phone: str, url: str, sources: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ ChatGPT å¾åŸå§‹è³‡è¨Šä¸­æå–çµæ§‹åŒ–è³‡æ–™"""
        
        # å¾åœ°å€è§£æç¸£å¸‚è³‡è¨Š
        prefecture = ""
        city = ""
        if "ç¦äº•çœŒ" in address:
            prefecture = "ç¦äº•çœŒ"
            # æå–å¸‚ç”ºæ‘è³‡è¨Š
            if "å¸‚" in address:
                city_part = address.replace("ç¦äº•çœŒ", "").split("å¸‚")[0] + "å¸‚"
            elif "ç”º" in address:
                city_part = address.replace("ç¦äº•çœŒ", "").split("ç”º")[0] + "ç”º"
            elif "æ‘" in address:
                city_part = address.replace("ç¦äº•çœŒ", "").split("æ‘")[0] + "æ‘"
            else:
                city_part = ""
            city = city_part
        
        # ç”Ÿæˆ geohash (ç°¡åŒ–ç‰ˆæœ¬)
        geohash = self._generate_geohash(lat, lon)
        
        # è™•ç†ä¾†æºè³‡è¨Š
        if sources is None:
            sources = []
        
        system_prompt = """ä½ æ˜¯ä¸€ä½è³‡æ–™åˆ†æå°ˆå®¶ï¼Œå°ˆé–€å¾æ–‡æœ¬ä¸­æå–çµæ§‹åŒ–è³‡è¨Šã€‚è«‹æ ¹æ“šæä¾›çš„ç¥ç¤¾è³‡è¨Šï¼Œæå–ä¸¦çµ„ç¹”æˆJSONæ ¼å¼çš„è³‡æ–™ã€‚
        
        **é‡è¦æé†’ï¼šæ—¥æœ¬æœ‰å¾ˆå¤šåŒåç¥ç¤¾ä½æ–¼ä¸åŒåœ°å€ï¼Œè«‹å‹™å¿…æ ¹æ“šæä¾›çš„å…·é«”åœ°å€ä¾†è­˜åˆ¥æ­£ç¢ºçš„ç¥ç¤¾ã€‚**
        
        è«‹ç‰¹åˆ¥æ³¨æ„ï¼š
        1. ç¢ºèªæ‰€æå–çš„è³‡è¨Šæ˜¯é‡å°æŒ‡å®šåœ°å€çš„ç¥ç¤¾ï¼Œä¸æ˜¯å…¶ä»–åœ°å€çš„åŒåç¥ç¤¾
        2. å¦‚æœæŸäº›è³‡è¨Šåœ¨æ–‡æœ¬ä¸­æ²’æœ‰æ˜ç¢ºæåŠï¼Œè«‹ä½¿ç”¨åˆç†çš„é è¨­å€¼æˆ–ç•™ç©ºå­—ä¸²
        3. å¹´ä»½è«‹ç›¡é‡æå–ï¼Œå¦‚æœä¸ç¢ºå®šè«‹ä½¿ç”¨ "ä¸æ˜"
        4. ç¥­ç¥è³‡è¨Šè«‹åŒ…å«ç¥æ˜åç¨±å’Œä¸»è¦åŠŸå¾·
        5. æ™‚é–“è³‡è¨Šè«‹æ¨™æº–åŒ–ç‚º24å°æ™‚åˆ¶æ ¼å¼ (ä¾‹å¦‚: "09:00-17:00")
        6. è²»ç”¨ä»¥æ—¥åœ“è¨ˆç®—ï¼Œå…è²»è«‹å¡«0
        7. å¸ƒæ—å€¼è«‹æ˜ç¢ºæ¨™ç¤º true/false
        8. é™£åˆ—å¦‚æœæ²’æœ‰è³‡è¨Šè«‹ä½¿ç”¨ç©ºé™£åˆ— []
        9. å¦‚æœæœå°‹çµæœèˆ‡æŒ‡å®šåœ°å€ä¸ç¬¦ï¼Œè«‹åœ¨æè¿°ä¸­è¨»æ˜
        
        è«‹åªå›å‚³JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—æˆ–èªªæ˜ã€‚"""
        
        # æ¨™æº–åŒ–ç¥ç¤¾åç¨±ï¼ˆç§»é™¤æ‹¬è™Ÿå…§çš„å‡åï¼‰
        normalized_shrine_name = normalize_shrine_name(shrine_name)
        
        user_prompt = f"""è«‹å¾ä»¥ä¸‹è³‡è¨Šä¸­æå–ç¥ç¤¾çš„çµæ§‹åŒ–è³‡æ–™ï¼š

ç¥ç¤¾åç¨±ï¼š{normalized_shrine_name}
åŸå§‹åç¨±ï¼š{shrine_name}
**ç›®æ¨™åœ°å€ï¼š{address}** (è«‹ç¢ºèªè³‡è¨Šä¾†æºæ˜¯é—œæ–¼æ­¤å…·é«”åœ°å€çš„ç¥ç¤¾)
ç·¯åº¦ï¼š{lat}
ç¶“åº¦ï¼š{lon}
é›»è©±ï¼š{phone}
ç¶²å€ï¼š{url}

è©³ç´°è³‡è¨Šï¼š
{raw_info}

åƒè€ƒä¾†æºè³‡è¨Šï¼š
{[source['title'] + ' - ' + source['url'] for source in sources][:5]}

**é‡è¦ï¼šè«‹ç¢ºèªæå–çš„è³‡è¨Šæ˜¯é—œæ–¼ä½æ–¼ã€Œ{address}ã€çš„{normalized_shrine_name}ï¼Œè€Œéå…¶ä»–åœ°å€çš„åŒåç¥ç¤¾ã€‚**

è«‹æå–ä»¥ä¸‹JSONçµæ§‹çš„è³‡æ–™ï¼š
{{
    "name_jp": "æ—¥æ–‡åç¨±",
    "name_en": "è‹±æ–‡åç¨±",
    "romaji": "ç¾…é¦¬æ‹¼éŸ³",
    "type": "ç¥ç¤¾æˆ–å¯º",
    "prefecture": "ç¸£å",
    "city": "å¸‚ç”ºæ‘å",
    "address": "å®Œæ•´åœ°å€",
    "lat": ç·¯åº¦æ•¸å€¼,
    "lon": ç¶“åº¦æ•¸å€¼,
    "geohash": "geohashå­—ä¸²",
    "nearest_station": "æœ€è¿‘è»Šç«™",
    "access_time_walk": "æ­¥è¡Œæ™‚é–“",
    "bus_info": "å·´å£«è³‡è¨Š",
    "parking": "åœè»Šè³‡è¨Š",
    "founded_year": "å‰µå»ºå¹´ä»½",
    "founder": "å‰µå»ºè€…",
    "historical_events": ["æ­·å²äº‹ä»¶é™£åˆ—"],
    "important_cultural_property": ["æ–‡åŒ–è²¡ç”¢é™£åˆ—"],
    "unesco": false,
    "architectural_style": "å»ºç¯‰æ¨£å¼",
    "enshrined_deities": [{{"name": "ç¥æ˜å", "role": "åŠŸå¾·"}}],
    "prayer_categories": ["ç¥ˆé¡˜é¡åˆ¥é™£åˆ—"],
    "omamori_types": ["å¾¡å®ˆç¨®é¡é™£åˆ—"],
    "goshuin": true,
    "ceremonies": [{{"name": "å„€å¼å", "reservation_req": true, "fee": é‡‘é¡}}],
    "gate_open": "é–‹é–€æ™‚é–“",
    "gate_close": "é—œé–€æ™‚é–“",
    "office_hours": "è¾¦å…¬æ™‚é–“",
    "admission_fee": 0,
    "annual_festivals": [{{"name": "ç¥­å…¸å", "date": "æ—¥æœŸ", "description": "æè¿°"}}],
    "highlights": ["çœ‹é»é™£åˆ—"],
    "best_seasons": ["æœ€ä½³å­£ç¯€é™£åˆ—"],
    "wheelchair_access": false,
    "toilets": true,
    "wifi": false,
    "photo_policy": "æ‹ç…§è¦å®š",
    "description": "ç¸½é«”æè¿°",
    "phone": "é›»è©±è™Ÿç¢¼",
    "url": "ç¶²å€",
    "sources": [{{"title": "ä¾†æºæ¨™é¡Œ", "url": "ä¾†æºç¶²å€", "snippet": "å…§å®¹æ‘˜è¦", "source": "ä¾†æºé¡å‹"}}]
}}"""
        
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=self.openai_headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content'].strip()
                # æ¸…ç†å¯èƒ½çš„markdownæ ¼å¼
                if content.startswith("```json"):
                    content = content[7:]
                if content.endswith("```"):
                    content = content[:-3]
                content = content.strip()
                
                try:
                    structured_data = json.loads(content)
                    # ç¢ºä¿ä½¿ç”¨æ¨™æº–åŒ–çš„ç¥ç¤¾åç¨±
                    if 'name_jp' in structured_data:
                        structured_data['name_jp'] = normalize_shrine_name(structured_data['name_jp'])
                    # ç¢ºä¿ geohash è¢«æ­£ç¢ºè¨­å®š
                    if not structured_data.get('geohash'):
                        structured_data['geohash'] = geohash
                    # ç¢ºä¿ä¾†æºè³‡è¨Šè¢«æ­£ç¢ºè¨­å®š
                    if sources:
                        structured_data['sources'] = sources
                    elif not structured_data.get('sources'):
                        structured_data['sources'] = []
                    return structured_data
                except json.JSONDecodeError as e:
                    print(f"JSON è§£æéŒ¯èª¤: {e}")
                    print(f"åŸå§‹å›æ‡‰: {content}")
                    return self._create_default_structure(shrine_name, address, lat, lon, phone, url, sources)
            else:
                return self._create_default_structure(shrine_name, address, lat, lon, phone, url, sources)
                
        except Exception as e:
            print(f"ChatGPT çµæ§‹åŒ–æå–éŒ¯èª¤: {e}")
            return self._create_default_structure(shrine_name, address, lat, lon, phone, url, sources)
    
    def _generate_geohash(self, lat: float, lon: float, precision: int = 8) -> str:
        """ç”Ÿæˆç°¡åŒ–ç‰ˆ geohash"""
        # ç°¡åŒ–çš„ geohash å¯¦ä½œ
        lat_str = f"{lat:.6f}"
        lon_str = f"{lon:.6f}"
        combined = f"{lat_str},{lon_str}"
        hash_obj = hashlib.md5(combined.encode())
        return hash_obj.hexdigest()[:precision]
    
    def _extract_location_info(self, address: str) -> str:
        """å¾åœ°å€ä¸­æå–é—œéµåœ°ç†è³‡è¨Š"""
        import re
        
        # æå–ç¸£
        prefecture = ""
        if "ç¦äº•çœŒ" in address:
            prefecture = "ç¦äº•çœŒ"
        elif "çœŒ" in address:
            prefecture_match = re.search(r'([^çœŒ]*çœŒ)', address)
            if prefecture_match:
                prefecture = prefecture_match.group(1)
        
        # æå–å¸‚/ç”º/æ‘
        city = ""
        if "å¸‚" in address:
            city_match = re.search(r'([^å¸‚]*å¸‚)', address)
            if city_match:
                city = city_match.group(1)
        elif "ç”º" in address:
            town_match = re.search(r'([^ç”º]*ç”º)', address)
            if town_match:
                city = town_match.group(1)
        elif "æ‘" in address:
            village_match = re.search(r'([^æ‘]*æ‘)', address)
            if village_match:
                city = village_match.group(1)
        
        # çµ„åˆé—œéµåœ°ç†è³‡è¨Š
        location_parts = []
        if prefecture:
            location_parts.append(prefecture)
        if city:
            location_parts.append(city)
        
        return " ".join(location_parts) if location_parts else ""
    
    def _is_relevant_result(self, content: str, target_address: str, city_info: str) -> bool:
        """æª¢æŸ¥æœå°‹çµæœæ˜¯å¦èˆ‡ç›®æ¨™åœ°å€ç›¸é—œ"""
        content_lower = content.lower()
        target_lower = target_address.lower()
        city_lower = city_info.lower()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™åœ°å€çš„é—œéµè©
        address_keywords = []
        
        # æå–åœ°å€ä¸­çš„é—œéµè©
        if "ç¦äº•" in target_address:
            address_keywords.extend(["ç¦äº•", "fukui"])
        if "å¸‚" in target_address:
            city_part = target_address.split("å¸‚")[0] + "å¸‚"
            if city_part:
                address_keywords.append(city_part.replace("ç¦äº•çœŒ", ""))
        
        # æª¢æŸ¥å…§å®¹æ˜¯å¦åŒ…å«é€™äº›é—œéµè©
        relevance_score = 0
        for keyword in address_keywords:
            if keyword.lower() in content_lower:
                relevance_score += 1
        
        # å¦‚æœåŒ…å«è‡³å°‘ä¸€å€‹åœ°å€é—œéµè©ï¼Œèªç‚ºæ˜¯ç›¸é—œçš„
        return relevance_score > 0 or city_lower in content_lower
    
    def _create_default_structure(self, name: str, address: str, lat: float, lon: float, phone: str, url: str, sources: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """å‰µå»ºé è¨­çš„è³‡æ–™çµæ§‹"""
        # å¾åœ°å€è§£æç¸£å¸‚è³‡è¨Š
        prefecture = ""
        city = ""
        if "ç¦äº•çœŒ" in address:
            prefecture = "ç¦äº•çœŒ"
            if "å¸‚" in address:
                city = address.replace("ç¦äº•çœŒ", "").split("å¸‚")[0] + "å¸‚"
            elif "ç”º" in address:
                city = address.replace("ç¦äº•çœŒ", "").split("ç”º")[0] + "ç”º"
            elif "æ‘" in address:
                city = address.replace("ç¦äº•çœŒ", "").split("æ‘")[0] + "æ‘"
        
        # è™•ç†ä¾†æºè³‡è¨Š
        if sources is None:
            sources = []
        
        return {
            "name_jp": normalize_shrine_name(name),
            "name_en": "",
            "romaji": "",
            "type": "ç¥ç¤¾" if "ç¥ç¤¾" in name else "å¯º",
            "prefecture": prefecture,
            "city": city,
            "address": address,
            "lat": lat,
            "lon": lon,
            "geohash": self._generate_geohash(lat, lon),
            "nearest_station": "",
            "access_time_walk": "",
            "bus_info": "",
            "parking": "",
            "founded_year": "ä¸æ˜",
            "founder": "",
            "historical_events": [],
            "important_cultural_property": [],
            "unesco": False,
            "architectural_style": "",
            "enshrined_deities": [],
            "prayer_categories": [],
            "omamori_types": [],
            "goshuin": True,
            "ceremonies": [],
            "gate_open": "",
            "gate_close": "",
            "office_hours": "",
            "admission_fee": 0,
            "annual_festivals": [],
            "highlights": [],
            "best_seasons": ["æ˜¥", "å¤", "ç§‹", "å†¬"],
            "wheelchair_access": False,
            "toilets": True,
            "wifi": False,
            "photo_policy": "ä¸€èˆ¬å…è¨±",
            "description": "",
            "phone": phone,
            "url": url,
            "sources": sources
        }

def load_shrine_data(csv_path: str) -> pd.DataFrame:
    """è¼‰å…¥ç¥ç¤¾è³‡æ–™"""
    try:
        df = pd.read_csv(csv_path)
        print(f"æˆåŠŸè¼‰å…¥ {len(df)} ç­†ç¥ç¤¾è³‡æ–™")
        return df
    except Exception as e:
        print(f"è¼‰å…¥è³‡æ–™éŒ¯èª¤: {e}")
        return pd.DataFrame()

def process_shrines(csv_path: str, output_path: str, num_shrines: Optional[int] = None) -> List[Dict[str, Any]]:
    """è™•ç†ç¥ç¤¾è³‡æ–™ä¸¦ç”¢ç”Ÿå¢å¼·ç‰ˆæœ¬ï¼ˆæ”¯æ´å¢é‡æ›´æ–°ï¼‰"""
    
    # è¼‰å…¥ CSV è³‡æ–™
    df = load_shrine_data(csv_path)
    if df.empty:
        return []
    
    # è¼‰å…¥ç¾æœ‰çš„ JSON è³‡æ–™
    print("ğŸ“‚ æª¢æŸ¥ç¾æœ‰ JSON è³‡æ–™...")
    existing_data = load_existing_json(output_path)
    processed_shrines = get_processed_shrines(existing_data)
    print(f"ğŸ” å·²è™•ç†ç¥ç¤¾: {len(processed_shrines)} ç­†")
    
    # åˆå§‹åŒ–å¢å¼·å™¨
    enhancer = ShrineDataEnhancer(PERPLEXITY_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY, GOOGLE_ENGINE_ID)
    
    # æ±ºå®šè™•ç†æ•¸é‡
    total_shrines = len(df)
    if num_shrines is None:
        num_shrines = total_shrines
    else:
        num_shrines = min(num_shrines, total_shrines)
    
    print(f"ğŸš€ é–‹å§‹è™•ç† {num_shrines} ç­†ç¥ç¤¾è³‡æ–™ï¼ˆç¸½å…± {total_shrines} ç­†ï¼‰")
    print(f"ğŸ’¡ å°‡è·³éå·²è™•ç†çš„ç¥ç¤¾ï¼Œåªè™•ç†æ–°è³‡æ–™")
    
    # è¨ˆæ•¸å™¨
    processed_count = 0
    skipped_count = 0
    new_data_count = 0
    
    # è™•ç†è³‡æ–™
    for idx in range(num_shrines):
        row = df.iloc[idx]
        shrine_name = row['ç¥ç¤¾åç¨±']
        address = row['ä½æ‰€']
        lat = float(row['ç·¯åº¦']) if pd.notna(row['ç·¯åº¦']) else 0.0
        lon = float(row['çµŒåº¦']) if pd.notna(row['çµŒåº¦']) else 0.0
        phone = row['é›»è©±ç•ªå·'] if pd.notna(row['é›»è©±ç•ªå·']) else ""
        url = row['URL'] if pd.notna(row['URL']) else ""
        
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†éï¼ˆä½¿ç”¨åœ°å€ä½œç‚ºå”¯ä¸€è­˜åˆ¥ï¼‰
        if address in processed_shrines:
            print(f"â­ï¸  ç¬¬ {idx + 1}/{num_shrines} ç­†: {shrine_name} - å·²è™•ç†ï¼Œè·³é")
            skipped_count += 1
            continue
        
        print(f"\nğŸ”„ è™•ç†ç¬¬ {idx + 1}/{num_shrines} ç­†: {shrine_name}")
        print(f"  ğŸ“ ç›®æ¨™åœ°å€: {address}")
        
        try:
            # 1. ä½¿ç”¨ç¶œåˆæœå°‹ï¼ˆPerplexity + Google Searchï¼‰
            print("  â†’ ç¶œåˆæœå°‹è©³ç´°è³‡è¨Šï¼ˆæ³¨é‡åœ°å€ç²¾ç¢ºæ€§ï¼‰...")
            search_results = enhancer.comprehensive_search(shrine_name, address)
            combined_info = search_results['combined_info']
            all_sources = search_results['all_sources']
            
            # 2. ä½¿ç”¨ ChatGPT æ½¤é£¾æè¿°
            print("  â†’ æ½¤é£¾æè¿°...")
            enhanced_description = enhancer.enhance_description_with_chatgpt(combined_info, shrine_name)
            
            # 3. æå–çµæ§‹åŒ–è³‡æ–™
            print("  â†’ æå–çµæ§‹åŒ–è³‡æ–™...")
            structured_data = enhancer.extract_structured_data_with_chatgpt(
                combined_info, shrine_name, address, lat, lon, phone, url, all_sources
            )
            
            # 4. æ›´æ–°æè¿°
            structured_data['description'] = enhanced_description
            
            # 5. ç«‹å³å„²å­˜åˆ° JSON æª”æ¡ˆ
            print("  â†’ å„²å­˜è³‡æ–™...")
            existing_data = save_single_shrine_incrementally(structured_data, output_path, existing_data)
            
            # æ›´æ–°å·²è™•ç†æ¸…å–®
            processed_shrines.add(address)
            processed_count += 1
            new_data_count += 1
            
            print(f"  âœ… å®Œæˆä¸¦å„²å­˜ ({processed_count} ç­†æ–°å¢, {skipped_count} ç­†è·³é)")
            
        except Exception as e:
            print(f"  âŒ è™•ç† {shrine_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
        
        # é¿å… API é™åˆ¶ï¼ŒåŠ å…¥å»¶é²
        print(f"  â³ ç­‰å¾…ä¸­...")
        time.sleep(1)  # ç¸®çŸ­ç­‰å¾…æ™‚é–“
    
    print(f"\nğŸ‰ è™•ç†å®Œæˆï¼")
    print(f"ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
    print(f"  - æ–°å¢è™•ç†: {new_data_count} ç­†")
    print(f"  - è·³éå·²è™•ç†: {skipped_count} ç­†") 
    print(f"  - ç¸½è¨ˆè³‡æ–™: {len(existing_data)} ç­†")
    
    return existing_data

def load_existing_json(output_path: str) -> List[Dict[str, Any]]:
    """è¼‰å…¥ç¾æœ‰çš„ JSON æª”æ¡ˆ"""
    try:
        if os.path.exists(output_path):
            with open(output_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… è¼‰å…¥ç¾æœ‰è³‡æ–™: {len(data)} ç­†")
            return data
        else:
            print("ğŸ“ æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°æª”æ¡ˆ")
            return []
    except Exception as e:
        print(f"âŒ è¼‰å…¥ç¾æœ‰æª”æ¡ˆéŒ¯èª¤: {e}")
        return []

def save_to_json(data: List[Dict[str, Any]], output_path: str):
    """å„²å­˜ç‚º JSON æª”æ¡ˆ"""
    try:
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è³‡æ–™å·²å„²å­˜è‡³: {output_path}")
    except Exception as e:
        print(f"âŒ å„²å­˜éŒ¯èª¤: {e}")

def save_single_shrine_incrementally(shrine_data: Dict[str, Any], output_path: str, existing_data: List[Dict[str, Any]]):
    """å¢é‡å„²å­˜å–®ç­†ç¥ç¤¾è³‡æ–™"""
    try:
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒç¥ç¤¾ï¼ˆæ ¹æ“šåœ°å€åˆ¤æ–·ï¼‰
        shrine_address = shrine_data.get('address', '')
        
        # æ›´æ–°ç¾æœ‰è³‡æ–™æˆ–æ–°å¢
        updated = False
        for i, existing_shrine in enumerate(existing_data):
            if existing_shrine.get('address') == shrine_address:
                existing_data[i] = shrine_data
                updated = True
                break
        
        if not updated:
            existing_data.append(shrine_data)
        
        # ç«‹å³å„²å­˜åˆ°æª”æ¡ˆ
        save_to_json(existing_data, output_path)
        
        return existing_data
    except Exception as e:
        print(f"âŒ å¢é‡å„²å­˜éŒ¯èª¤: {e}")
        return existing_data

def get_processed_shrines(existing_data: List[Dict[str, Any]]) -> set:
    """å–å¾—å·²è™•ç†çš„ç¥ç¤¾æ¸…å–®ï¼ˆæ ¹æ“šåœ°å€ä½œç‚ºå”¯ä¸€è­˜åˆ¥ï¼‰"""
    processed = set()
    for shrine in existing_data:
        address = shrine.get('address', '')
        if address:
            processed.add(address)  # ä½¿ç”¨å®Œæ•´åœ°å€ä½œç‚ºå”¯ä¸€è­˜åˆ¥ç¬¦
    return processed

def normalize_shrine_name(name: str) -> str:
    """æ¨™æº–åŒ–ç¥ç¤¾åç¨±ï¼Œç§»é™¤æ‹¬è™Ÿå…§çš„å‡å"""
    import re
    # ç§»é™¤æ‹¬è™Ÿå’Œæ‹¬è™Ÿå…§çš„å…§å®¹
    normalized = re.sub(r'ï¼ˆ[^ï¼‰]*ï¼‰', '', name)
    normalized = re.sub(r'\([^)]*\)', '', normalized)
    return normalized.strip()

if __name__ == "__main__":
    # è¨­å®šè·¯å¾‘
    csv_path = "/Users/zhuboyuan/Desktop/University-NCHU/NCHU-Project/Project-FUKUI/src/src-LLM-Shrine/data/shrines_detail.csv"
    output_path = "/Users/zhuboyuan/Desktop/University-NCHU/NCHU-Project/Project-FUKUI/src/src-LLM-Shrine/output/enhanced_shrines_full.json"
    
    print("ğŸ¯ ç¦äº•ç¥ç¤¾è³‡æ–™å¢å¼·ç¨‹å¼")
    print("=" * 50)
    print("ğŸ” ä½¿ç”¨ Perplexity API é€²è¡Œæ™ºèƒ½æœå°‹")
    print("ğŸŒ ä½¿ç”¨ Google Custom Search é€²è¡Œç¶²è·¯æœå°‹")
    print("ğŸ¤– ä½¿ç”¨ GPT-4o-mini é€²è¡Œè³‡æ–™æ½¤é£¾å’Œçµæ§‹åŒ–")
    print("ğŸ“š åŒ…å«å®Œæ•´ä¾†æºç¶²å€è¿½æº¯åŠŸèƒ½")
    print("ğŸ’¾ æ”¯æ´å¢é‡æ›´æ–°ï¼Œæ¯ç­†è³‡æ–™è™•ç†å®Œç«‹å³å„²å­˜")
    print()
    
    # è™•ç†æ‰€æœ‰ç¥ç¤¾è³‡æ–™ï¼ˆä¸é™åˆ¶æ•¸é‡ï¼‰
    enhanced_data = process_shrines(csv_path, output_path, num_shrines=None)
    
    if enhanced_data:
        # é¡¯ç¤ºç¯„ä¾‹
        print(f"\nğŸ“Š è™•ç†å®Œæˆï¼å…±å¢å¼· {len(enhanced_data)} ç­†ç¥ç¤¾è³‡æ–™")
        print("\nğŸ“‹ ç¯„ä¾‹è³‡æ–™çµæ§‹:")
        if enhanced_data:
            sample = enhanced_data[0]
            for key, value in list(sample.items())[:10]:  # åªé¡¯ç¤ºå‰10å€‹æ¬„ä½
                print(f"  {key}: {value}")
            print("  ...")
            
        print(f"\nğŸ“ å®Œæ•´è³‡æ–™å·²å„²å­˜è‡³: {output_path}")
        print("ğŸ¯ å¯ç”¨æ–¼å‰ç«¯ä»‹é¢çš„ JSON æ ¼å¼å·²æº–å‚™å®Œæˆ")
            
    else:
        print("âŒ æ²’æœ‰è™•ç†ä»»ä½•è³‡æ–™")
